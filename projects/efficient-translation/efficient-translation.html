<!DOCTYPE html>
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
      onload="renderMathInElement(document.body);"></script>
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      prefix: 'tw-',
    };
  </script>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>



<head>

  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Efficient fine-tuning and inference of LLMs for English–Indonesian translation using LoRA and QLoRA under resource constraints.">
  <meta property="og:title" content="Efficient Translation of LLMs"/>
  <meta property="og:description" content="Exploring PEFT techniques like QLoRA for efficient LLM training on limited GPU.">
  <meta property="og:url" content="https://kaindraa.github.io/projects/efficient-translation/efficient-translation.html"/>

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
  <!-- <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by -->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Efficient Translation of LLMs</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  

<link rel="stylesheet" href="static/css/style.css">


</style>
<script>
document.addEventListener('DOMContentLoaded', () => {

  bulmaCarousel.attach('#results-carousel',{
    slidesToScroll : 1,
    slidesToShow   : 1,
    loop           : true,
    infinite       : true,
    autoplay       : true,
    autoplaySpeed  : 5000,
    icons:{
      previous:`<svg viewBox="0 0 24 24">
                  <polyline points="15 6 9 12 15 18"
                            stroke="currentColor"
                            stroke-linecap="round" stroke-linejoin="round"/>
                </svg>`,
      next:`<svg viewBox="0 0 24 24">
              <polyline points="9 6 15 12 9 18"
                        stroke="currentColor"
                        stroke-linecap="round" stroke-linejoin="round"/>
            </svg>`
    }
  });

});
</script>



</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Efficient Fine-Tuning and Inference of LLMs under Resource Constraints for English–Indonesian Translation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="kaindraa.github.io" target="_blank">Kaindra Rizq Sachio</a><sup>*</sup></span>

                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Universitas Indonesia<br>May 2025</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <!-- <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span> -->
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Results Figure Carousel -->
<!-- Results Figure Carousel -->
<section class="hero teaser">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div id="results-carousel" class="carousel results-carousel" style="text-align: center;">

        <div class="item">
          <img src="static/figures/sacrebleu_all.png" style="width: 70%; height: auto; display: block; margin: 0 auto;">
          <p class="subtitle mt-4">SacreBLEU scores for all configurations. QLoRA 3B reaches <strong>32.80</strong>.</p>
        </div>

      <div class="item">
        <img src="static/figures/loravsqlora_trainingtime.png" style="width: 70%; height: auto; display: block; margin: 0 auto;" alt="Training Time">
        <p class="subtitle mt-4">
          Training time comparison between LoRA and QLoRA.<br>
          QLoRA incurs <strong>27–35%</strong> longer training time depending on the model size.<br>
          <small class="has-text-grey">*The percentages reflect QLoRA’s performance relative to LoRA.</small>
        </p>
      </div>

      <div class="item">
        <img src="static/figures/loravsqlora_peaktrainingmemory.png" style="width: 70%; height: auto; display: block; margin: 0 auto;" alt="Peak Training Memory">
        <p class="subtitle mt-4">
          Peak memory usage during training: QLoRA reduces memory by <strong>16%–53%</strong> compared to LoRA.<br>
          <small class="has-text-grey">*The percentages reflect QLoRA’s performance relative to LoRA.</small>
        </p>
      </div>

        <div class="item">
          <img src="static/figures/loravsqlora_bleudrop.png" style="width: 70%; height: auto; display: block; margin: 0 auto;" alt="BLEU Drop">
          <p class="subtitle mt-4">
            BLEU drop after quantization. QLoRA is more stable in preserving translation quality after quantization.
          </p>
        </div>

        <div class="item">
          <img src="static/figures/inferencetime_perconfig.png" style="width: 70%; height: auto; display: block; margin: 0 auto;" alt="Inference Time">
          <p class="subtitle mt-4">Inference time increases up to 155.59% with 4-bit quantized models.</p>
        </div>

        <div class="item">
          <img src="static/figures/peakinferencememory_perconfig.png" style="width: 70%; height: auto; display: block; margin: 0 auto;" alt="Inference Memory">
          <p class="subtitle mt-4">Peak memory during inference is not reduced significantly by 4-bit quantization.</p>
        </div>
      </div>


      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Proyek ini mengeksplorasi teknik <em>fine-tuning</em> dan <em>inference</em> yang efisien dalam model <em>Large Language Model</em> (LLM) dalam kondisi sumber daya terbatas, dengan fokus pada <em>Parameter-Efficient Fine-Tuning</em> (PEFT) seperti LoRA dan QLoRA, serta optimasi <em>inference</em> melalui <em>quantization</em> 4-bit. Eksperimen ini membandingkan performa model Qwen2.5 0.5B, 1.5B, dan 3B pada tugas penerjemahan bahasa Inggris ke Indonesia menggunakan dataset FLORES-200, dengan menilai waktu training, <em>peak memory</em>, dan skor SacreBLEU pada format <em>full precision</em> maupun versi 4-bit. Dibanding <em>full fine-tuning</em>, LoRA dan QLoRA secara signifikan mengurangi kebutuhan <em>peak memory</em> saat training, dengan pengurangan mencapai <strong>44.50%</strong> pada model 0.5B dan skor SacreBLEU yang jauh lebih baik. Dibanding LoRA, QLoRA memberikan efisiensi <em>peak memory</em> yang lebih tinggi, hingga <strong>53.52%</strong> lebih rendah pada model 3B, meskipun waktu training meningkat sekitar <strong>27–35%</strong>. Konfigurasi QLoRA 3B terbukti paling optimal dengan skor SacreBLEU tertinggi (<strong>32.80</strong>) dan performa yang stabil setelah <em>quantization</em>. Pada tahap <em>inference</em>, meskipun model 4-bit meningkatkan waktu eksekusi secara signifikan (hingga <strong>155.59%</strong>), tidak ditemukan pengurangan <em>peak memory</em> <em>inference</em> yang berarti.
          <div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section" id="Introduction">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="introduction">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Belakangan ini, Large Language Models (LLM) telah menunjukkan kemampuan yang sangat baik dalam berbagai tugas Natural Language Processing (NLP). Kemampuannya juga semakin berkembang dengan Instruction Tuning, yang membuat model bisa memahami dan mengikuti instruksi dalam bahasa alami. Namun, banyak LLM yang memiliki jumlah parameter sangat besar, seringkali di atas 1B. Melakukan full fine tuning di model dengan ukuran yang besar akan sangat berat, apalagi dalam kondisi dengan resource terbatas.
          </p>
          <p>
            Oleh karena itu, project ini akan mendemonstrasikan, mengevaluasi, dan membandingkan teknik yang dapat digunakan di resources constrained LLM yakni LoRA dan QLoRA. Selain itu, optimasi dalam inference juga dibahas, karena hal ini penting ketika model sudah di-deploy dan harus menangani banyak request sekaligus.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- SECTION: Methodology -->
<section class="section" id="methodology">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Methodology</h2>

        <!-- Subsection: Dataset and Task -->
        <h3 class="title is-4">Dataset and Task</h3>
        <div class="content has-text-justified">
          <p>
            Dataset yang digunakan adalah 
            <a class="has-text-link" href="https://arxiv.org/abs/2207.04672" target="_blank">
              FLORES-200 (Goyal et al., 2022)
            </a>, yang dirilis oleh Meta AI pada 30 Juni 2022. Dataset ini mencakup 200 bahasa berbeda dan dibuat dari terjemahan 842 artikel web berbeda.
          </p>
          <p>
            Task yang digunakan dalam project ini adalah penerjemahan dari English ke Bahasa Indonesia. Jadi, hanya subset pasangan English–Indonesian dari dataset ini yang diambil.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="section" id="metric">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">

        <h3 class="title is-4">Metric</h3>
        <div class="content has-text-justified">

          <!-- Penjelasan awal -->
        <div class="content has-text-justified">
          <p>
            Metrik yang akan digunakan adalah 
            <a class="has-text-link" href="https://aclanthology.org/W18-6319/" target="_blank">SacreBLEU (Post, 2018) </a>, 
            yakni versi yang lebih stabil dan <em>reproducible</em> dari BLEU klasik 
            (<a class="has-text-link" href="https://aclanthology.org/P02-1040/" target="_blank">Papineni et al., 2002</a>). 
            BLEU sendiri adalah algoritma yang menilai kualitas hasil terjemahan dari suatu bahasa ke bahasa lain 
            dengan mengukur kemiripan (<em>overlap</em>) dengan teks referensi, berdasarkan kemunculan n-gram yang sama.
          </p>

          <p>
            SacreBLEU juga mengikuti prinsip yang sama seperti BLEU, tetapi dengan beberapa perbaikan penting. 
            Pertama, SacreBLEU menggunakan tokenizer standar yang konsisten, sehingga hasil evaluasi lebih <em>reproducible</em>. 
            Selain itu, SacreBLEU juga menghindari preprocessing manual yang tidak transparan. 
            <strong>Secara rumus perhitungan, SacreBLEU sama saja dengan BLEU, yang membedakannya adalah implementasi praktisnya</strong>.<br><br>BLEU sendiri terdiri dari beberapa komponen utama yang bekerja sama untuk menghitung skor akhir:
          </p>
        </div>


<div class="content has-text-justified">

  <!-- Modified n-gram Precision -->
  <div class="box has-background-info-light" style="margin-bottom: 1rem;">
    <p><strong>Modified n-gram Precision</strong></p>
    <p>
      Bagian ini menghitung seberapa banyak n-gram dari kalimat hasil terjemahan (candidate) cocok dengan n-gram dari satu atau lebih kalimat referensi (reference). 
      Maksud <em>modified</em> di sini adalah dengan melakukan clip, sehingga tidak menghitung dobel n-gram yang muncul lebih sering daripada referensi.
    </p>
    <p>$$
    p_n = \frac{ \sum_{\text{ngram} \in \text{cand}} \min\left(\text{Count}_{\text{cand}}(\text{ngram}), \text{MaxRefCount}(\text{ngram})\right) }{ \sum_{\text{ngram} \in \text{cand}} \text{Count}_{\text{cand}}(\text{ngram}) }
    $$</p>
  </div>

  <!-- Brevity Penalty -->
  <div class="box has-background-info-light" style="margin-bottom: 1rem;">
    <p><strong>Brevity Penalty</strong></p>
    <p>
      Brevity Penalty (BP) digunakan untuk mencegah model menghasilkan terjemahan yang terlalu pendek.
      Jika panjang hasil terjemahan \(c\) lebih pendek dari referensi \(r\), maka skor akhir akan dikalikan penalti.
    </p>
    <p>$$
    \text{BP} =
    \begin{cases}
    1 & \text{jika } c > r \\\\
    \exp\left(1 - \frac{r}{c}\right) & \text{jika } c \leq r
    \end{cases}
    $$</p>
  </div>

  <!-- BLEU Score -->
  <div class="box has-background-info-light" style="margin-bottom: 1rem;">
    <p><strong>BLEU Score</strong></p>
    <p>
      Skor akhir BLEU menggabungkan semua precision dari n-gram (1-gram sampai 4-gram) menggunakan rata-rata logaritmik.
      Hasilnya kemudian dikalikan dengan Brevity Penalty untuk menghukum output yang terlalu pendek.
    </p>
    <p>$$
    \text{BLEU} = \text{BP} \cdot \exp\left( \sum_{n=1}^{N} w_n \cdot \log p_n \right)
    $$</p>
  </div>

</div>



<h3 class="title is-4">Model</h3>
<div class="content has-text-justified">

  <p>
    Model yang digunakan adalah 
    <a class="has-text-link" href="https://arxiv.org/abs/2412.15115" class="has-text-link" target="_blank">
      Qwen2.5 (Yang et al., 2024)
    </a> 
    dalam versi Base 0.5B, 1.5B, dan 3B. Model ini sudah dipretraining di berbagai bahasa termasuk Indonesia. 
    Yang digunakan adalah versi <em>base</em> (bukan <em>instruct</em>) agar evaluasi tidak terpengaruh oleh tuning sebelumnya.
  </p>

  <p>
    Model 7B tidak digunakan karena environment hanya memiliki 24GB VRAM, sedangkan model 7B membutuhkan sekitar 26.08 GB dalam FP32.
  </p>

<details class="code-block">
  <summary>Code</summary>
  <pre><code class="language-python">def load_qwen_model(model_type, nf4 = False):
    model_names = {
        '0.5B': "Qwen/Qwen2.5-0.5B",
        '1.5B': "Qwen/Qwen2.5-1.5B",
        '3B':  "Qwen/Qwen2.5-3B",
        '7B':  "Qwen/Qwen2.5-7B"
    }
    if model_type not in model_names:
        raise ValueError(f"Define size model type tidak sesuai: {model_type}")
    if nf4:
        nf4_config = BitsAndBytesConfig(
           load_in_4bit=True,
           bnb_4bit_quant_type="nf4",
           bnb_4bit_use_double_quant=True,
           bnb_4bit_compute_dtype=torch.bfloat16
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_names[model_type], quantization_config=nf4_config
        ).to("cuda")
    else:
        model = AutoModelForCausalLM.from_pretrained(model_names[model_type]).to("cuda")
    model.config.use_cache = False
    return model</code></pre>

</details>
</div>


<h3 class="title is-4">GPU</h3>
<div class="content has-text-justified">
  <p>
    Training ini menggunakan 1 x NVIDIA RTX 4090 dengan VRAM sebesar 24GB. GPU ini disewa melalui platform <a href="https://vast.ai" target="_blank">vast.ai</a>.
  </p>
</div>
</section>


<!-- SECTION: Methodology -->
<section class="section" id="Experimental Setup and Implementation">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experimental Setup and Implementation</h2>

        <!-- Subsection: Dataset and Task -->
        <h3 class="title is-4">Instruction Tuning</h3>
        <div class="content has-text-justified">
          <p>
<p>
  Instruction tuning adalah salah satu teknik Fine-tuning dimana model dilatih agar dapat mengikuti bahasa manusia. Instruction tuning digunakan karena ini adalah pendekatan utama yang membuat model LLM mampu merespons instruksi dalam format percakapan atau perintah, seperti "terjemahkan kalimat ini" atau "buat ringkasan dari teks berikut." Hampir semua model LLM saat ini seperti ChatGPT, LLaMA 2 chat, atau Qwen Instruct dilatih menggunakan instruction tuning agar bisa digunakan secara interaktif oleh pengguna.
</p>

<p>
  Meskipun project ini hanya melakukan satu <em>task</em>, yakni terjemahan, saya tetap ambil pendekatan ini untuk mensimulasikan skenario training instruction tuning.
</p>

<p>
  Prompt template yang digunakan adalah alpaca, model akan diberi prompt dari 
  <code class="i-code">### Instruction</code> 
  hingga 
  <code class="i-code">### Response:</code>, 
  sementara target-nya adalah teks setelah 
  <code class="i-code">### Response:</code>. 
  Untuk memastikan bahwa hanya bagian completion yang dihitung loss maka saya menggunakan 
  <code class="i-code">DataCollatorForCompletionOnlyLM</code>.
</p>

          </p>
        </div>
<details class="code-block">
  <summary>Code</summary>
  <pre><code class="language-python">EOS_TOKEN = tokenizer.eos_token

alpaca_prompt = """

### Instruction:
Translate this English text into Indonesian.

### Input:
{}

### Response:
{}
"""

def format_prompt(examples):
    inputs = examples["english"]
    outputs = examples["indonesian"]
    texts = []
    for input, output in zip(inputs, outputs):
        text = alpaca_prompt.format(input, output) + EOS_TOKEN
        texts.append(text)
    return {"text": texts}

response_template = "### Response:\n"
collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer)

ds_train = ds_train.map(format_prompt, batched=True)
ds_valid = ds_valid.map(format_prompt, batched=True)
```</code></pre>

</details>

 <!-- Subsection: Training Setup -->
        <h3 class="title is-4"><br><br>Training Setup</h3>
        <div class="content has-text-justified">
<p>
  Agar perbandingan <em>time</em> dan <em>memory</em> adil, semua model menggunakan 1 epoch dan training batch size 2 (ini batch size terbesar yang muat untuk training LoRA 3B).
  <code class="i-code">Packing = False</code> digunakan agar sesuai dengan persyaratan dari
  <code class="i-code">DataCollatorForCompletionOnlyLM</code> yakni mencegah input digabung menjadi panjang.
  <code class="i-code">learning_rate</code> yang digunakan berada di rentang umum yakni <code class="i-code">5e-4</code>.
</p>


 </div>
<details class="code-block">
  <summary>Code</summary>
  <pre><code class="language-python">training_args = SFTConfig(
    output_dir=output_dir,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=1,
    learning_rate=5e-4,
    num_train_epochs=1,
    seed=123,
    packing=False,
    save_strategy="no",

    fp16=True,
    optim="paged_adamw_8bit" if experiment_type == "qlora" else "adamw_torch",
    eval_strategy='steps',
    eval_steps=50,
    logging_steps=50,
    report_to="wandb",
    run_name=f"{experiment_type}_{model_type}_2",
        )</code></pre>

</details>

<!-- Subsection: Full Fine Tuning -->
<h3 class="title is-4"><br><br>Full Fine Tuning</h3>
<div class="content has-text-justified">
  <p>
    Full fine tuning adalah proses fine tuning dari LLM dimana semua parameter dari model dilatih. Ini adalah metode training yang paling menggunakan sumber daya.
  </p>

  <p>
    Saya dapat melakukan aproksimasi memori yang dibutuhkan untuk melakukan <strong>full fine-tuning</strong> pada model LLM sebagai berikut. Misalkan saya memiliki model LLM dengan total parameter sebesar <code class="i-code">B</code>. Selama proses training, terdapat beberapa komponen tambahan yang harus dialokasikan dalam memori, yaitu:
  </p>

  <table class="table is-fullwidth is-bordered is-striped">
    <thead>
      <tr>
        <th>Komponen</th>
        <th>Faktor terhadap jumlah parameter</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>Parameter model (weights)</td><td>B</td></tr>
      <tr><td>Gradien</td><td>B</td></tr>
      <tr><td>Optimizer states (Adam)</td><td>2B</td></tr>
      <tr><td>Aktivasi dan variabel sementara</td><td>2B</td></tr>
      <tr><td><strong>Total</strong></td><td><strong>6B</strong></td></tr>
    </tbody>
  </table>

  <p>
    Untuk menghitung total memori GPU yang diperlukan, kita kalikan total parameter efektif (<code class="i-code">6B</code>) dengan ukuran dalam byte per parameter, lalu dikonversi ke gigabyte (GB) menggunakan rumus:
  </p>

  <pre><code>Memory (GB) = 6 * B * BytesPerParameter / 1024^3</code></pre>

  <p>
    Pembagian dengan \(1024^3\) digunakan untuk mengubah satuan dari byte ke gigabyte, karena:
  </p>

  <pre><code>1 GB = 1024^3 bytes = 1,073,741,824 bytes</code></pre>

  <p>
    Dengan rumus ini, kita dapat memperkirakan kebutuhan memori dari berbagai model:
  </p>

  <table class="table is-fullwidth is-bordered is-striped">
    <thead>
      <tr>
        <th>Jumlah Parameter (B)</th>
        <th>Estimasi Memory (GB, FP32)</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>0.5B</td><td>11.18 GB</td></tr>
      <tr><td>1.5B</td><td>33.53 GB</td></tr>
      <tr><td>3B</td><td>67.06 GB</td></tr>
      <tr><td>7B</td><td>156.46 GB</td></tr>
    </tbody>
  </table>

  <p>
    Dapat dilihat bahwa hanya model berukuran 0.5B yang dapat dilatih dengan full fine tuning di GPU dengan memory 24 GB.
  </p>
</div>

<details class="code-block">
  <summary>Code</summary>
  <pre><code class="language-python">trainer = SFTTrainer(
    model=model,
    args=training_args,
    dataset_text_field='text',
    train_dataset=ds_train,
    data_collator=collator,
    tokenizer=tokenizer,
    eval_dataset=ds_valid,

)</code></pre>

</details>
</div>
</div>
        <h3 class="title is-4">LoRA</h3>
        <div class="content has-text-justified">

          <p>
            Low-Rank Adaptation (LoRA)
            <a class="has-text-link" href="https://arxiv.org/abs/2106.09685" target="_blank">(Hu et al., 2021)</a>
            adalah salah satu teknik Parameter Efficient Fine Tuning (PEFT) dengan cara hanya melatih sebagian kecil parameter yang disebut Low Rank Matrices.
          </p>

          <p>
            Misal dalam task ini, \(x_i\) adalah Prompt dari instruction tuning, \(y_i\) adalah completion-nya, \(\mathcal{Z}\) adalah pasangan instruction-completion 
            \(\mathcal{Z} = \{(x_i,y_i)\}_{i=1,...,N}\), \(t\) adalah urutan token, dan \(\Phi_0\) adalah bobot model pre-trained. 
            Dalam training, model akan diupdate menjadi \(\Phi_0+\Delta\Phi\) secara berulang untuk memaksimalkan conditional language modelling objektif.
          </p>

        <div class="box has-background-info-light" style="overflow-x: auto; margin: 1rem 0;">
          <div>$$
          \max_{\Phi} \sum_{(x,y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log \left( P_{\Phi}(y_t|x,y_{\lt t}) \right)
          $$</div>
        </div>

          <p>
            Permasalahan dari full fine-tuning ini adalah LLM memiliki jumlah parameter \(\Phi_0\) yang sangat besar dan juga berlaku \(\Phi_0 = \Delta\Phi\).
          </p>

          <p>
            LoRA memberikan teknik yang lebih hemat memori dengan cara melakukan modifikasi 
            \(\Delta\Phi = \Delta\Phi(\Theta)\) dengan \(\Theta\) adalah parameter tambahan berukuran kecil 
            \(|\Theta| \ll |\Phi_0|\). Maka task sekarang menjadi mencari \(\Delta\Phi(\Theta)\) dengan mengoptimasi \(\Theta\).
          </p>

        <div class="box has-background-info-light" style="overflow-x: auto; margin: 1rem 0;">
          <div>$$
          \max_{\Theta} \sum_{(x,y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log \left( P_{\Phi_0+\Delta\Phi(\Theta)}(y_t|x,y_{\lt t}) \right)
          $$</div>
        </div>

          <p>
            Penulis paper LoRA berhipotesis bahwa update dari pre-trained matriks \(\Phi_0 \in \mathbb{R}^{d \times k}\)
            dapat dibatasi ke bentuk dekomposisi low rank dengan \(\Delta\Phi = BA\),
            di mana \(B \in \mathbb{R}^{d \times r}\), \(A \in \mathbb{R}^{r \times k}\), dan \(r\) adalah hyperparameter bernama rank.
          </p>

          <p>
            Untuk melakukan training dengan LoRA, langkah pertama adalah membuat 
            <code class="i-code">lora_config</code> dari library HuggingFace. Setelah itu, konfigurasi tersebut dihubungkan ke model dengan memanggil 
            <code class="i-code">get_peft_model()</code>, yang akan menambahkan adapter LoRA ke model utama.
          </p>

        </div>

 <h3 class="title is-4">QLoRA</h3>
<div class="content has-text-justified">
  <p>
    QLoRA (<a class="has-text-link" href="https://arxiv.org/abs/2305.14314" target="_blank">Dettmers et al., 2023</a>) adalah evolusi dari LoRA yang menerapkan berbagai teknik baru untuk lebih mengecilkan memory. Ada 4 Resep dari QLoRA:
  </p>

  <!-- Resep 1 -->
  <div class="box has-background-info-light">
    <p><strong>Resep 1: Normal Float 4bit (NF4) Quantization</strong></p>
    <p>
      Normalnya, komputer merepresentasikan setiap angka dari parameter, optimizer, activation, dll dalam 32 bit (Full Precision).
      Dengan quantization, kita representasikan angka menjadi lebih sedikit bit (16, 8, 4, dst). Teknik ini hemat memori namun dengan trade-off presisi.
    </p>
    <p>
      QLoRA menggunakan Normal Float 4bit (NF4), di mana distribusi angka mengikuti Normal distribution karena banyak angka berada di sekitar nol.
    </p>
  </div>

  <!-- Resep 2 -->
  <div class="box has-background-info-light">
    <p><strong>Resep 2: Double Quantization</strong></p>
    <p>
      Menyimpan bobot 4-bit memerlukan scaling factor, dan scaling factor ini bisa makan banyak memori. Dengan double quantization,
      scaling factor tersebut juga di-quantize.
    </p>
    <p>
      Resep 1 dan 2 diimplementasikan lewat <code class="i-code">BitsAndBytesConfig</code>:
    </p>
    <details class="code-block">
      <summary>Code</summary>
      <pre><code class="language-python">if nf4:
    nf4_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16
    )</code></pre>
    </details>
  </div>

  <!-- Resep 3 -->
  <div class="box has-background-info-light">
    <p><strong>Resep 3: Paged Optimizers</strong></p>
    <p>
      Paged optimizer menyimpan optimizer state yang tidak aktif di RAM, dan hanya load yang aktif ke VRAM.
    </p>
    <p>
      Diterapkan melalui konfigurasi <code class="i-code">SFTConfig</code>:
    </p>
    <details class="code-block">
      <summary>Code</summary>
      <pre><code class="language-python">training_args = SFTConfig(
    ...
    optim="paged_adamw_8bit" if experiment_type == "qlora" else "adamw_torch",
    ...
)</code></pre>
    </details>
  </div>

  <!-- Resep 4 -->
  <div class="box has-background-info-light">
    <p><strong>Resep 4: LoRA + Semua Resep Sebelumnya</strong></p>
    <p>
      QLoRA juga mencakup LoRA itu sendiri yang dikombinasikan dengan 3 resep sebelumnya.
    </p>
  </div>

  <!-- Warning -->
  <div class="notification" style="background-color: rgba(239, 20, 20, 0.15);"; color: #7c5300;">
    <strong>Warning</strong><br>
    Saya sempat melakukan satu kesalahan fatal saat menggunakan QLoRA. Sebelumnya, saya langsung melakukan merge adapter ke model yang sudah diquantize 4 bit.
    Padahal, adapter LoRA dilatih dengan presisi yang lebih tinggi (FP16), sehingga jika langsung digabung ke model 4-bit, maka bobot ΔW dalam presisi yang rendah.
    Akibatnya, banyak informasi penting dari presisi digit yang hilang saat merging.
    <br><br>
    Setelah menyadari hal ini, saya mengubah pendekatan: saya memuat model base dalam presisi tinggi (FP32), lalu merge adapter, kemudian quantize ulang ke 4-bit jika dibutuhkan.
  </div>
</div>

<!-- Subsection: Quantized Inference -->
<h3 class="title is-4">Quantized Inference</h3>
<div class="content has-text-justified">
  <p>
    Saat model dideploy, quantization bisa jadi pilihan efektif untuk mengurangi beban komputasi dan penggunaan memori. 
    Untuk melihat dampaknya, evaluasi juga dilakukan pada model yang telah diquantize ke format 4-bit. 
    Secara hanya penyimpanan, format ini hanya membutuhkan sekitar <strong>12.5%</strong> memori dibandingkan model full precision (<code class="i-code">FP32</code>), 
    sehingga cocok untuk inference di environment dengan memori terbatas.
  </p>
</div>

<!-- Subsection: Evaluation -->
<h3 class="title is-4">Evaluation</h3>
<div class="content has-text-justified">
  <p>
    Evaluasi dilakukan terhadap beberapa konfigurasi model: full fine-tuning untuk model 0.5B, serta LoRA dan QLoRA untuk model 0.5B, 1.5B, dan 3B. 
    Setiap konfigurasi dinilai berdasarkan tiga aspek utama:
  </p>
  <ul>
    <li><strong>SacreBLEU</strong>: untuk mengukur kualitas hasil terjemahan, baik dalam format normal maupun setelah model diquantize ke 4-bit.</li>
    <li><strong>Time</strong>: Menghitung untuk waktu training atau waktu inference (baik untuk model normal maupun versi 4-bit).</li>
    <li><strong>Peak Memory</strong>: Penggunaan memori maksimum selama training dan inference, termasuk saat menggunakan model dalam format 4-bit.</li>
  </ul>
  <p>
    Evaluasi ini bertujuan untuk memahami perbandingan trade-off antara efisiensi memori, kecepatan komputasi, dan kualitas hasil terjemahan dalam berbagai pendekatan fine-tuning di lingkungan dengan resource terbatas.
    <strong> Batch size yang digunakan dalam evaluasi adalah 32.</strong>
  </p>

  <details class="code-block">
    <summary>Code</summary>
    <pre><code class="language-python">
# Script evaluasi inference dan BLEU scoring
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
from tqdm import tqdm
import evaluate
import time
import tempfile
import os
import shutil
import pandas as pd
from peft import PeftModel

def run_evaluation(model=None, tokenizer=None, ds_valid=None, 
                   batch_size=32, print_decoded=False, 
                   do_return=True, num_samples=None, quantization_4bit=False, is_qlora=False, model_type=None, print_limit=5):
    torch.cuda.empty_cache()

    # Inference untuk model quantized 4-bit
    if quantization_4bit:
        tmpdirname = tempfile.mkdtemp()

        if isinstance(model, PeftModel):
            model = model.merge_and_unload()

        model.save_pretrained(tmpdirname)
        tokenizer.save_pretrained(tmpdirname)

        quant_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )

        model = AutoModelForCausalLM.from_pretrained(
            tmpdirname, trust_remote_code=True, quantization_config=quant_config
        ).to("cuda")

        tokenizer = AutoTokenizer.from_pretrained(tmpdirname, trust_remote_code=True)
        shutil.rmtree(tmpdirname)

    # Inference untuk model QLoRA (merge adapter ke base)
    elif not quantization_4bit and is_qlora:
        tmpdirname = tempfile.mkdtemp()

        model.save_pretrained(tmpdirname, save_adapter=True)
        tokenizer.save_pretrained(tmpdirname)
        del model
        torch.cuda.empty_cache()

        base_model = load_qwen_model(model_type)
        model = PeftModel.from_pretrained(base_model, tmpdirname)
        model = model.merge_and_unload()
        model.to("cuda")

        shutil.rmtree(tmpdirname)

    # Inference untuk model biasa
    else:
        if isinstance(model, PeftModel):
            model = model.merge_and_unload()
        model.to("cuda")

    # Subset dataset jika num_samples diberikan
    if num_samples is not None:
        ds_valid = ds_valid.select(range(min(num_samples, len(ds_valid))))

    metric = evaluate.load("sacrebleu")
    predictions = []
    references = []
    prompts_all = []
    decoded_all = []
    printed_count = 0

    start = time.time()

    # Loop inference batch
    for i in tqdm(range(0, len(ds_valid), batch_size)):
        batch = ds_valid[i:i+batch_size]
        prompts = []
        batch_references = []

        for full in batch["text"]:
            if "### Response:\n" in full:
                prompt, response = full.split("### Response:\n", 1)
                prompt = prompt + "### Response:\n"
                prompts.append(prompt)
                batch_references.append(response.replace("<|endoftext|>", ""))

        if not prompts:
            continue

        inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to("cuda")

        with torch.no_grad():
            outputs = model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                max_new_tokens=128,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id,
            )

        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)

        for out, prompt, ref in zip(decoded_outputs, prompts, batch_references):
            decoded = out[len(prompt):]
            predictions.append(decoded)
            references.append(ref)
            prompts_all.append(prompt)
            decoded_all.append(decoded)

            if print_decoded and printed_count < print_limit:
                print(f'Prompt -> {prompt}')
                print(f'Ref -> {ref}')
                print(f'Decoded -> {decoded}')
                print('-' * 75)
                printed_count += 1

    end = time.time()
    inference_time = round(end - start, 2)

    result = metric.compute(predictions=predictions, references=[[r] for r in references])
    bleu = round(result["score"], 2)

    print("BLEU score:", bleu)
    print("Inference time (s):", inference_time)

    df_output = pd.DataFrame({
        "prompt": prompts_all,
        "reference": references,
        "decoded": decoded_all
    })

    if do_return:
        return {
            "bleu": bleu,
            "inference_time": inference_time,
            "predictions": predictions,
            "references": references,
            "df_output": df_output
        }
    </code></pre>
  </details>
</div>


      </div>
    </div>
  </div>  
</section>

<section class="section" id="result-and-analysis">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Result and Analysis</h2>
        <div class="table-container">
          <table class="table is-striped is-bordered is-fullwidth is-hoverable">
            <thead>
              <tr>
                <th>Configuration</th>
                <th>SacreBLEU</th>
                <th>SacreBLEU 4-bit</th>
                <th>Training Time (s)</th>
                <th>Inference Time (s)</th>
                <th>Inference Time 4-bit (s)</th>
                <th>Peak Training Memory (GB)</th>
                <th>Peak Inference Memory (GB)</th>
                <th>Peak Inference 4-bit Memory (GB)</th>
              </tr>
            </thead>
            <tbody>
              <tr><td>Full 3B</td><td>–</td><td>–</td><td>–</td><td>–</td><td>–</td><td>–</td><td>–</td><td>–</td></tr>
              <tr><td>LoRA 3B</td><td>32.73</td><td>31.49</td><td>189.10</td><td>34.16</td><td>71.19</td><td>19.45</td><td>12.07</td><td>14.72</td></tr>
              <tr><td>QLoRA 3B</td><td><strong>32.80</strong></td><td><strong>31.97</strong></td><td>241.76</td><td>33.18</td><td>77.37</td><td>9.04</td><td>13.97</td><td>13.49</td></tr>
              <tr><td>Full 1.5B</td><td>–</td><td>–</td><td>–</td><td>–</td><td>–</td><td>–</td><td>–</td><td>–</td></tr>
              <tr><td>LoRA 1.5B</td><td>26.74</td><td>25.54</td><td>135.71</td><td>25.31</td><td>53.79</td><td>12.89</td><td>6.21</td><td>7.45</td></tr>
              <tr><td>QLoRA 1.5B</td><td>27.13</td><td>26.92</td><td>180.18</td><td><strong>23.70</strong></td><td>59.35</td><td>8.19</td><td>7.27</td><td>6.88</td></tr>
              <tr><td>Full 0.5B</td><td>0.20</td><td>0.20</td><td>134.55</td><td>24.33</td><td><strong>43.32</strong></td><td>13.55</td><td>6.61</td><td>6.30</td></tr>
              <tr><td>LoRA 0.5B</td><td>15.59</td><td>12.26</td><td><strong>107.18</strong></td><td>26.78</td><td>68.42</td><td>8.96</td><td><strong>2.09</strong></td><td>2.63</td></tr>
              <tr><td>QLoRA 0.5B</td><td>14.54</td><td>13.65</td><td>145.54</td><td>30.96</td><td>59.35</td><td><strong>7.52</strong></td><td>2.50</td><td><strong>1.21</strong></td></tr>
            </tbody>
          </table>
        </div>

  <h3 class="title is-4">Konfigurasi Terbaik</h3>
<div class="content has-text-justified">
  <figure style="text-align: center;">
    <img src="static/figures/sacrebleu_all.png" alt="SacreBLEU All" style="max-width: 80%; height: auto; margin-bottom: 0.5rem;">
    <figcaption><em>Figure 1: Skor SacreBLEU di semua konfigurasi</em></figcaption>
  </figure>

  <p>
    Performa model meningkat secara konsisten seiring bertambahnya jumlah parameter.
    <strong>QLoRA 3B</strong> memiliki performa SacreBLEU tertinggi (32.80) dan efisiensi memori training yang sangat baik
    dengan hanya menggunakan 9GB memori saat training.
    Model ini merupakan pilihan paling optimal untuk environment GPU RTX 4090 ini.
  </p>
</div>


<h3 class="title is-4">Full vs PEFT (LoRA & QLoRA)</h3>
<div class="content has-text-justified">
  <p>
    Pada training model 0.5B, <strong>LoRA</strong> mengurangi <em>peak memory</em> sebesar <strong>&darr;33.87%</strong>, sementara <strong>QLoRA</strong> sebesar <strong>&darr;44.50%</strong>.
    LoRA juga mengurangi waktu pelatihan sebesar <strong>&darr;20.34%</strong>, tetapi QLoRA justru meningkat sedikit sebesar <strong>&uarr;7.87%</strong>.
  </p>

  <p>
    SacreBLEU pada Full 0.5B sangat rendah (hanya <strong>0.20</strong>), menunjukkan bahwa model tidak berhasil belajar dengan baik.
    Hal ini kemungkinan disebabkan karena proses training dengan ukuran batch yang kecil tidak stabil dengan update banyak parameter sekaligus.
  </p>

  <p>
    Salah satu hal menarik yang saya temukan adalah penggunaan memori saat inference pada model full fine-tuning 0.5B cenderung lebih tinggi dibandingkan konfigurasi LoRA atau QLoRA, baik dalam versi normal maupun 4-bit.
    Padahal, secara teori, seharusnya semua model akan serupa saat inference karena adapter sudah di-<em>merge</em> dan tidak lagi aktif.
  </p>
</div>

<h3 class="title is-4">LoRA vs QLoRA</h3>
<div class="content has-text-justified">
  <p><strong>1. SacreBLEU</strong></p>
  <p>
    Untuk inference normal, SacreBLEU LoRA lebih tinggi untuk model dengan jumlah parameter rendah (0.5B: 15.59 vs 14.54),
    sementara untuk model dengan jumlah parameter lebih tinggi, SacreBLEU QLoRA cenderung lebih baik dengan hanya perbedaan sedikit
    (1.5B: 26.74 vs 27.13, 3B: 32.73 vs 32.80).
  </p>
  <p>
    Dalam kondisi quantized (4-bit), QLoRA secara konsisten lebih stabil. Penurunan BLEU pada QLoRA hanya <strong>6.12%</strong> di model 0.5B,
    jauh lebih kecil dibanding LoRA yang turun hingga <strong>21.35%</strong>.
    Ini menunjukkan keunggulan pendekatan <em>Quantization-Aware Training</em> yang digunakan QLoRA,
    karena model dilatih sambil mempertimbangkan efek dari quantization sejak awal.
  </p>

  <p><strong>2. Training Time</strong></p>
  <figure style="text-align: center;">
    <img src="static/figures/loravsqlora_trainingtime.png" alt="LoRA vs QLoRA Training Time" style="max-width: 80%;" />
    <figcaption><em>Figure 2: Perbandingan training time LoRA vs QLoRA. Persentase yang ditunjukkan adalah performa QLoRA dibandingkan LoRA</em></figcaption>
  </figure>
  <p>
    QLoRA membutuhkan waktu training lebih lama dibanding LoRA di semua ukuran model:
    <strong>&uarr;34.79%</strong> (0.5B), <strong>&uarr;32.76%</strong> (1.5B), dan <strong>&uarr;27.84%</strong> (3B).
    Tren ini menunjukkan bahwa overhead dari QLoRA bersifat relatif tetap, sehingga pengaruhnya terhadap total waktu semakin kecil pada model yang lebih besar.
  </p>

  <p><strong>3. Peak Training Memory</strong></p>
  <figure style="text-align: center;">
    <img src="static/figures/loravsqlora_peaktrainingmemory.png" alt="LoRA vs QLoRA Peak Memory" style="max-width: 80%;" />
    <figcaption><em>Figure 3: Perbandingan Peak Training Memory LoRA vs QLoRA. Persentase yang ditunjukkan adalah performa QLoRA dibandingkan LoRA</em></figcaption>
  </figure>
  <p>
    QLoRA secara signifikan lebih hemat memori dibanding LoRA. Pengurangan penggunaan memori training QLoRA dibanding LoRA tercatat sebesar
    <strong>&darr;16.07%</strong> (0.5B), <strong>&darr;36.46%</strong> (1.5B), dan <strong>&darr;53.52%</strong> (3B).
    Semakin besar ukuran model, semakin besar pula efisiensi memori yang diperoleh. Hal ini menunjukkan bahwa overhead QLoRA tidak bertambah secara proporsional
    dengan jumlah parameter, tidak seperti pada LoRA.
  </p>

  <p><strong>4. QLoRA as Quantization-Aware Training</strong></p>
  <figure style="text-align: center;">
    <img src="static/figures/loravsqlora_bleudrop.png" alt="LoRA vs QLoRA SacreBLEU Drop" style="max-width: 80%;" />
    <figcaption><em>Figure 4: Perbandingan penurunan SacreBLEU setelah quantization 4-bit</em></figcaption>
  </figure>
  <p>
    QLoRA menunjukkan penurunan SacreBLEU yang lebih kecil dibanding LoRA setelah quantization ke 4-bit saat inference.
    Penurunan SacreBLEU untuk masing-masing ukuran model:
  </p>
  <ul>
    <li>0.5B: LoRA <strong>&darr;21.35%</strong>, QLoRA <strong>&darr;6.12%</strong></li>
    <li>1.5B: LoRA <strong>&darr;4.48%</strong>, QLoRA <strong>&darr;0.77%</strong></li>
    <li>3B: LoRA <strong>&darr;3.78%</strong>, QLoRA <strong>&darr;2.53%</strong></li>
  </ul>
  <p>
    Stabilitas QLoRA ini disebabkan oleh pendekatannya yang bersifat quantization-aware.
    Karena model dilatih langsung dalam kondisi quantized (menggunakan representasi NF4),
    proses training secara eksplisit menyesuaikan diri terhadap degradasi presisi.
    Hal ini menjelaskan mengapa selisih performa antara model normal dan model 4-bit pada QLoRA jauh lebih kecil dibanding LoRA,
    yang tidak mempertimbangkan quantization saat training.
  </p>
</div>

<h3 class="title is-4">FP32 vs 4-bit Inference</h3>
<div class="content has-text-justified">
  <p>
    Secara konsisten, model quantized menghasilkan SacreBLEU yang lebih rendah dibanding model full precision. 
    Penurunan kualitas ini terjadi di semua konfigurasi dan mencerminkan <em>trade-off</em> antara efisiensi dan akurasi saat menggunakan quantization.
  </p>

  <figure style="text-align: center;">
    <img src="static/figures/inferencetime_perconfig.png" alt="FP32 vs 4-bit Inference Time" style="max-width: 80%;" />
    <figcaption><em>Figure 5: Perbandingan Inference Time FP32 vs 4-bit untuk semua konfigurasi.</em></figcaption>
  </figure>

  <p>
    Namun, ada hal menarik pada inference time. Alih-alih lebih cepat, model quantized justru membutuhkan waktu lebih lama saat inference. 
    Peningkatan time terendah adalah Full 0.5B dengan (<strong>&uarr;78.05%</strong>) dan peningkatan tertinggi ada di LoRA 0.5B (<strong>&uarr;155.59%</strong>). 
    Setelah melakukan research mengapa ini terjadi, karena proses dequantization yang terjadi saat runtime. 
    Berdasarkan detail teknis dari QLoRA 
    <a class="has-text-link" href="https://arxiv.org/abs/2305.14314" target="_blank">(Dettmers et al., 2023)</a>, 
    ketika <code class="i-code">load_in_4bit=True</code>, layer linear dalam model akan diquantize menggunakan format NF4, 
    lalu secara dinamis didequantize kembali ke <code class="i-code">bnb_compute_dtype</code> (dalam project ini <code class="i-code">torch.bfloat16</code>) 
    selama proses inference. Proses quantize dan dequantize dalam layer ini menambah overhead komputasi.
  </p>

  <figure style="text-align: center;">
    <img src="static/figures/peakinferencememory_perconfig.png" alt="FP32 vs 4-bit Peak Inference Memory" style="max-width: 80%;" />
    <figcaption><em>Figure 6: Perbandingan Peak Inference Memory FP32 vs 4-bit untuk semua konfigurasi.</em></figcaption>
  </figure>

  <p>
    Selain itu, penggunaan memori selama inference 4-bit tidak menunjukkan pengurangan signifikan dibanding versi normal. 
    Bahkan, dalam beberapa kasus seperti LoRA 3B, puncak memori saat inference 4-bit justru lebih tinggi. 
    Saya menduga ini masih berkaitan dengan proses quantize-dequantize dinamis, yang memerlukan buffer tambahan selama runtime.
  </p>
</div>

<h3 class="title is-4">Contoh Inference</h3>
<div class="content has-text-justified">
  <p>
    Berikut contoh inference dari konfigurasi terbaik model <strong>3B QLoRA</strong>:
  </p>

<div class="box" style="background-color: #a6a6a6;">
  <strong>### Instruction:</strong><br>
  Translate this English text into Indonesian.<br><br>

  <strong>### Input:</strong><br>
  When you went abroad at first, people were probably patient and understanding, knowing that travellers in a new country need to adapt.<br><br>

  <strong>### Response:</strong>
</div>


  <p>Completion-nya adalah:</p>

  <div class="box" style="background-color: #9bd3f7;">
    <strong>Reference:</strong> "Saat Anda pergi ke luar negeri pertama kali, orang-orang mungkin sabar dan pengertian karena mengetahui bahwa wisatawan di negara baru perlu beradaptasi."
  </div>

  <div class="box" style="background-color: #f6d9ae;">
    <strong>Prediction:</strong> "Ketika Anda pergi ke luar negeri pertama kali, mungkin orang-orang itu bersabar dan mengerti, mengetahui bahwa wisatawan di negara baru membutuhkan adaptasi."
  </div>

  <div class="box" style="background-color: #a6f5c2;">
    <strong>Prediction 4-bit:</strong> "Ketika Anda pergi ke luar negeri pertama kali, orang-orang mungkin bersabar dan paham, mengetahui bahwa wisatawan di negara baru perlu mengadaptasi."
  </div>

  <p>
    Model mampu menghasilkan terjemahan yang sangat baik pada beberapa sampel. Perlu dicatat bahwa dataset ini dikonstruksi menggunakan hasil terjemahan sebagai referensi, sehingga kualitas referensinya tidak sepenuhnya natural atau representatif seperti teks asli.
    Selain itu, versi quantized 4-bit tidak menunjukkan penurunan kualitas yang signifikan dibanding versi normal, dan hasil prediksinya masih cukup bagus.
  </p>
</div>

      </div>
    </div>
  </div>


</section>


<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code></code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
